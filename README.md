# Intelligent Acoustic Monitoring: AI-Based Analysis of Microphone Variability in Natural Environments

This project uses a conditional Generative Adverserial Network (cGAN) to augment spectrograms from low end microphones with features from high fidelity devices. The model is trained on audio captured during a rewilding project.

## Usage

Ensure raw data is formatted as

```python
raw_data/
├── full_summaries/
│   ├── loc2_summary/
│   └── loc1_summary/
└── year/
    ├── microphone_1/
    │   ├── location1/
    │   │   └──location_YYYYMMDD_HHMMSS.wav
    │   └── location2/
    │   │   └──location_YYYYMMDD_HHMMSS.wav
    └── microphone2/
        ├── location1/
        └── location2/
```

Running the main script will create the dataset, train the model and run the evaluation pipeline automatically.

Hyperparameters are easily modified from the configuration file.

It's better to let `create_dataset` generate matching time and date summaries for each microphone folder.
They are generated using filenames which must be formatted as `LOCATION_YYYYMMDD_HHMMSS.wav` To guarantee their creation do not add summary folders to microphone directories. The dataset creation pipeline supports these files however any mistakes with filename/formatting will propagate through the program and can cause unexpected behaviour.

Recordings from the target microphone must include a delimiter so generated and target audio can be matched later,
for example input from the SMMicro has the filename `PLI1_20231104_125433.wav`. The target microphone, SM4, has a name with the delimiter `-4` included: `PLI1-4_20231104_125433.wav`.

Full summary files should be kept in a folder `full_summaries` just under the parent directory. This allows the evaluation steps to find the latitude and longitude of each recording, and allows extensibility in the future - if more columns are added, more analysis can be done.

The output will be some audio analysis, the generated summary folders, a JSON file displaying the directories used and how they're organised, and a train/val/test set with the % of training data defined in the configuration file. The test data folder also includes a params directory, that has magnitude and phase information, along with some other values, that'll influence how audio is recomposed.

Dataset images are a composition of input and target images, with the training sample on the left and the target on the right.

While training, the model runs through some validation data. Images generated by the model during training and validation are saved in the dataset/train_runs folder. Generated, input and target images are saved as a trio, with a sample per epoch to stay memory efficient. Peak signal to noise ratio and structural similarity index measure are used to compare the quality of generated and tarfet images throughout training.

No downsampling has been used on the images, and they are padded to fit into the models size requirements. The dimensions must be in the series $2^n$. Images are cropped and normalised before being saved to save space and have a realistic output.

 Images generated from the test set are automatically saved. Evaluation also transforms a generated spectrogram into audio, using the targets magnitude and phase. This allows computing acoustic indices, which are measures that enable quantifiable comparisons of the generated and target images.

Pairs of generated and target audio are also run through BirdNet so eco-acoustic quality is compared.
