# Intelligent Acoustic Monitoring: AI-Based Analysis of Microphone Variability in Natural Environments

This project uses a conditional Generative Adverserial Network (cGAN) to learn differences between [high-end](https://www.wildlifeacoustics.com/products/song-meter-sm4) and [low-end](https://www.wildlifeacoustics.com/products/song-meter-micro) SM microphones used in a rewilding project. The model aims to learn how to translate audio from the cheaper microphone to the more expensive one.

By leveraging machine learning techniques, this study aims to identify differences in species detection and ambient noise between these microphones when deployed in a forest setting. This research provides insight into the suitability of different types of microphones for ecological monitoring and the potential impact on data collection in environmental studies, potentially broadening possibilities for high quality species monitoring with reduced expense.

A cGAN has two components, the discriminator and the generator. The generator learns the mapping between input and target images well enough to make the discriminator classify them as real.

## Notes

Generating small spectrograms make longer spectrograms squash and may increase inaccuracy. Would recommend not to use `correlated` if so.

## Usage

Ensure raw data is formatted as

```python
raw_data/
├── full_summaries/
│   ├── loc2_summary/
│   └── loc1_summary/
└── year/
    ├── microphone_1/
    │   ├── location1/
    │   │   └──location_YYYYMMDD_HHMMSS.wav
    │   └── location2/
    │   │   └──location_YYYYMMDD_HHMMSS.wav
    └── microphone2/
        ├── location1/
        └── location2/
```

Navigate to the `code` directory and run `python create_dataset.py` to create the dataset automagically.

It's better to let `create_dataset` generate matching time and date summaries for each microphone folder.
They are generated using filenames which must be formatted as `LOCATION_YYYYMMDD_HHMMSS.wav` To guarantee their creation do not add summary folders to each microphone directory. The dataset creation pipeline supports these files however any mistakes with filename/formatting will propagate through the program and can cause unexpected behaviour.

Recordings from the target microphone must include a delimiter so generated and target audio can be matched later,
for example input from the SMMicro has the filename `PLI1_20231104_125433.wav`. The target microphone, SM4, has a name with the delimiter `-4` included: `PLI1-4_20231104_125433.wav`.

Full summary files should be kept in a folder `full_summaries` just under the parent directory. This allows the evaluation steps to find
the latitude and longitude of each recording, and allows extensibility in the future - if more columns are added, more analysis can be done.

The output will be some audio analysis, the generated summary folders, a JSON file displaying the directories used and how they're organised, and a train/val/test set with the % of training data defined in the configuration file. The test data folder also includes a params directory, that has magnitude and phase information, along with some other values, that'll influence how audio is recomposed.

Dataset images are a composition of input and target images, with the training sample on the left and the target on the right.

To train the model run `python train_model.py`. This will use the dataset and immediatly run training. After each epoch, the model runs through some validation data. Images generated by the model during training and validation are saved in the dataset/train_runs folder. Generated, input and target images are saved as a trio, with a sample per epoch to stay memory efficient. Peak signal to noise ratio and structural similarity index measure are used to compare the quality of generated and tarfet images throughout training.

No downsampling has been used on the images, and they are padded to fit into the models size requirements. The dimensions must be in the series $2^n$. Images are cropped and normalised before being saved to save space and have a realistic output.

Once training is completed, evaluation can be run using `python evaluate_model.py`.

The model runs through the test set and generated images to show its performance after training. Evaluation also transforms a generated spectrogram into audio, using the targets magnitude and phase. This allows computing acoustic indices, which are measures that enable quantifiable comparisons of the generated and target images.

Pairs of generated and target audio are also run through BirdNet so eco-acoustic quality is compared.
